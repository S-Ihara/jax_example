{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import optax \n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n"
     ]
    }
   ],
   "source": [
    "# gpu確認\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下flaxによるCIFAR10実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random key\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    }
   ],
   "source": [
    "# モデル\n",
    "# flaxではモデルはモデルの定義のみで実際のパラメータは別で持っている\n",
    "# -> @nn.compactのせい\n",
    "# 多分普通にpytorchっぽくも書けるっぽい？\n",
    "class SimpleCNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self,x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3,3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2,2), strides=(2,2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3,3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2,2), strides=(2,2))\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x\n",
    "\n",
    "# モデルのチェックなど\n",
    "model = SimpleCNN()\n",
    "x = jnp.ones((3, 32, 32, 3))\n",
    "params = model.init(key, jnp.ones([1,32,32,3]))['params']\n",
    "out = model.apply({'params': params}, x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data mean [0.49139968 0.48215841 0.44653091]\n",
      "Data std [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "# データセット\n",
    "# torchvisionからCIFAR10を用いる\n",
    "DATASET_PATH = \"./cache\"\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
    "print(\"Data mean\", DATA_MEANS)\n",
    "print(\"Data std\", DATA_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image util functions\n",
    "def image_to_numpy(img):\n",
    "    \"\"\"\n",
    "    PIL形式の画像をnumpy形式に変換して、正規化する関数\n",
    "    \"\"\"\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255. - DATA_MEANS) / DATA_STD\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = image_to_numpy\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "    image_to_numpy\n",
    "])\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "    collate_fn=numpy_collate, num_workers=num_workers, persistent_workers=True)\n",
    "val_loader   = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "    collate_fn=numpy_collate, num_workers=num_workers, persistent_workers=True)\n",
    "test_loader  = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "    collate_fn=numpy_collate, num_workers=num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    batch_stats: Any\n",
    "\n",
    "def create_train_state(key, learning_rate, momentum):\n",
    "    model = SimpleCNN()\n",
    "    input_shape = (1, 32, 32, 3)\n",
    "    params = model.init(key, jnp.ones(input_shape))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの生成\n",
    "key, init_key = jax.random.split(key, 2)\n",
    "model_state = create_train_state(init_key, 0.1, 0.9)\n",
    "#params = model.init(init_key, jnp.ones(input_shape))['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3220553\n",
      "acc 0.0546875\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss_acc(state,params,batch):\n",
    "    \"\"\"\n",
    "    Loss: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    data_inputs, labels = batch\n",
    "    #labels = jnp.array(labels)\n",
    "    logits = state.apply_fn({\"params\": params}, data_inputs)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "    acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "loss,acc = calculate_loss_acc(model_state, model_state.params, batch)\n",
    "print(\"loss\", loss)\n",
    "print(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3220487\n",
      "acc 0.0546875\n",
      "acc 0.234375\n"
     ]
    }
   ],
   "source": [
    "# train function\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss\n",
    "                                 argnums=1,  # Parameters are second argument of the function\n",
    "                                 has_aux=True  # Function has additional outputs, here accuracy\n",
    "                                )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, acc), grads = grad_fn(state, state.params, batch)\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss, acc\n",
    "\n",
    "# eval function\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc\n",
    "\n",
    "# train test\n",
    "model_state, loss, acc = train_step(model_state, batch)\n",
    "print(\"loss\", loss)\n",
    "print(\"acc\", acc)\n",
    "\n",
    "# eval test\n",
    "acc = eval_step(model_state, batch)\n",
    "print(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train_Loss: 1.5358785390853882, Train Acc: 0.4463363587856293, Val Acc: 0.5376952886581421\n",
      "Epoch: 1, Train_Loss: 1.2157222032546997, Train Acc: 0.5678864121437073, Val Acc: 0.613085925579071\n",
      "Epoch: 2, Train_Loss: 1.0992904901504517, Train Acc: 0.6159188151359558, Val Acc: 0.6373046636581421\n",
      "Epoch: 3, Train_Loss: 1.0180217027664185, Train Acc: 0.64507657289505, Val Acc: 0.6519531011581421\n",
      "Epoch: 4, Train_Loss: 0.9694800972938538, Train Acc: 0.6638621687889099, Val Acc: 0.665234386920929\n",
      "Epoch: 5, Train_Loss: 0.8968766927719116, Train Acc: 0.6904380321502686, Val Acc: 0.669726550579071\n",
      "Epoch: 6, Train_Loss: 0.8641999363899231, Train Acc: 0.7015447020530701, Val Acc: 0.662890613079071\n",
      "Epoch: 7, Train_Loss: 0.8321110010147095, Train Acc: 0.7086894512176514, Val Acc: 0.67578125\n",
      "Epoch: 8, Train_Loss: 0.8040343523025513, Train Acc: 0.7223334908485413, Val Acc: 0.6832031011581421\n",
      "Epoch: 9, Train_Loss: 0.7632994055747986, Train Acc: 0.7381588220596313, Val Acc: 0.6962890625\n",
      "Epoch: 10, Train_Loss: 0.7557587027549744, Train Acc: 0.74040687084198, Val Acc: 0.697070300579071\n",
      "Epoch: 11, Train_Loss: 0.7251332402229309, Train Acc: 0.7518696784973145, Val Acc: 0.7085937261581421\n",
      "Epoch: 12, Train_Loss: 0.7025906443595886, Train Acc: 0.757478654384613, Val Acc: 0.7007812261581421\n",
      "Epoch: 13, Train_Loss: 0.6899540424346924, Train Acc: 0.7638888955116272, Val Acc: 0.698437511920929\n",
      "Epoch: 14, Train_Loss: 0.6634080410003662, Train Acc: 0.77254718542099, Val Acc: 0.7074218988418579\n",
      "Epoch: 15, Train_Loss: 0.6552855372428894, Train Acc: 0.7741720080375671, Val Acc: 0.710742175579071\n",
      "Epoch: 16, Train_Loss: 0.6386659145355225, Train Acc: 0.7814725637435913, Val Acc: 0.7085937261581421\n",
      "Epoch: 17, Train_Loss: 0.6265063285827637, Train Acc: 0.7838318943977356, Val Acc: 0.6900390386581421\n",
      "Epoch: 18, Train_Loss: 0.6153767704963684, Train Acc: 0.7925124764442444, Val Acc: 0.7124999761581421\n",
      "Epoch: 19, Train_Loss: 0.6046295166015625, Train Acc: 0.7938256859779358, Val Acc: 0.71875\n",
      "Epoch: 20, Train_Loss: 0.5927867293357849, Train Acc: 0.7976985573768616, Val Acc: 0.7162109613418579\n",
      "Epoch: 21, Train_Loss: 0.5799682140350342, Train Acc: 0.8019275069236755, Val Acc: 0.70703125\n",
      "Epoch: 22, Train_Loss: 0.5732870697975159, Train Acc: 0.8051326274871826, Val Acc: 0.7074218988418579\n",
      "Epoch: 23, Train_Loss: 0.5577279329299927, Train Acc: 0.8105857968330383, Val Acc: 0.7193359136581421\n",
      "Epoch: 24, Train_Loss: 0.5520812273025513, Train Acc: 0.8148593306541443, Val Acc: 0.7181640863418579\n",
      "Epoch: 25, Train_Loss: 0.5451197028160095, Train Acc: 0.813724160194397, Val Acc: 0.725781261920929\n",
      "Epoch: 26, Train_Loss: 0.537719190120697, Train Acc: 0.8196670413017273, Val Acc: 0.7232421636581421\n",
      "Epoch: 27, Train_Loss: 0.5292121767997742, Train Acc: 0.8217815160751343, Val Acc: 0.724804699420929\n",
      "Epoch: 28, Train_Loss: 0.520625114440918, Train Acc: 0.8252092003822327, Val Acc: 0.733593761920929\n",
      "Epoch: 29, Train_Loss: 0.509620189666748, Train Acc: 0.8294827342033386, Val Acc: 0.7074218988418579\n",
      "Epoch: 30, Train_Loss: 0.5012952089309692, Train Acc: 0.83237624168396, Val Acc: 0.7197265625\n",
      "Epoch: 31, Train_Loss: 0.5180290341377258, Train Acc: 0.8271456360816956, Val Acc: 0.713671863079071\n",
      "Epoch: 32, Train_Loss: 0.4901174306869507, Train Acc: 0.8359375, Val Acc: 0.729296863079071\n",
      "Epoch: 33, Train_Loss: 0.48814693093299866, Train Acc: 0.8360487818717957, Val Acc: 0.7333984375\n",
      "Epoch: 34, Train_Loss: 0.4831114411354065, Train Acc: 0.8388310074806213, Val Acc: 0.725781261920929\n",
      "Epoch: 35, Train_Loss: 0.4847886860370636, Train Acc: 0.838964581489563, Val Acc: 0.724414050579071\n",
      "Epoch: 36, Train_Loss: 0.4843384921550751, Train Acc: 0.8394097089767456, Val Acc: 0.716015636920929\n",
      "Epoch: 37, Train_Loss: 0.4788972735404968, Train Acc: 0.8387197256088257, Val Acc: 0.7201172113418579\n",
      "Epoch: 38, Train_Loss: 0.4778924882411957, Train Acc: 0.8402777910232544, Val Acc: 0.7212890386581421\n",
      "Epoch: 39, Train_Loss: 0.48948875069618225, Train Acc: 0.8379406929016113, Val Acc: 0.7216796875\n",
      "Epoch: 40, Train_Loss: 0.4628768861293793, Train Acc: 0.8475783467292786, Val Acc: 0.7210937738418579\n",
      "Epoch: 41, Train_Loss: 0.46280142664909363, Train Acc: 0.8464431762695312, Val Acc: 0.7095702886581421\n",
      "Epoch: 42, Train_Loss: 0.4497602880001068, Train Acc: 0.8518295884132385, Val Acc: 0.7330077886581421\n",
      "Epoch: 43, Train_Loss: 0.4540730118751526, Train Acc: 0.8511841297149658, Val Acc: 0.716601550579071\n",
      "Epoch: 44, Train_Loss: 0.4609999358654022, Train Acc: 0.8487357497215271, Val Acc: 0.7261718511581421\n",
      "Epoch: 45, Train_Loss: 0.445360392332077, Train Acc: 0.8532763719558716, Val Acc: 0.731249988079071\n",
      "Epoch: 46, Train_Loss: 0.44324544072151184, Train Acc: 0.855457603931427, Val Acc: 0.718945324420929\n",
      "Epoch: 47, Train_Loss: 0.4398973286151886, Train Acc: 0.855479896068573, Val Acc: 0.732226550579071\n",
      "Epoch: 48, Train_Loss: 0.4470684230327606, Train Acc: 0.8548789024353027, Val Acc: 0.7164062261581421\n",
      "Epoch: 49, Train_Loss: 0.43456509709358215, Train Acc: 0.8587740659713745, Val Acc: 0.7269531488418579\n",
      "Epoch: 50, Train_Loss: 0.44665488600730896, Train Acc: 0.8548343777656555, Val Acc: 0.7152343988418579\n",
      "Epoch: 51, Train_Loss: 0.44211864471435547, Train Acc: 0.8562144041061401, Val Acc: 0.722460925579071\n",
      "Epoch: 52, Train_Loss: 0.4422118067741394, Train Acc: 0.85670405626297, Val Acc: 0.712695300579071\n",
      "Epoch: 53, Train_Loss: 0.4318116009235382, Train Acc: 0.8609775900840759, Val Acc: 0.715039074420929\n",
      "Epoch: 54, Train_Loss: 0.4442538321018219, Train Acc: 0.8573050498962402, Val Acc: 0.715039074420929\n",
      "Epoch: 55, Train_Loss: 0.4293515980243683, Train Acc: 0.8611333966255188, Val Acc: 0.692578136920929\n",
      "Epoch: 56, Train_Loss: 0.4321531653404236, Train Acc: 0.8607549667358398, Val Acc: 0.728320300579071\n",
      "Epoch: 57, Train_Loss: 0.4326901435852051, Train Acc: 0.8603543639183044, Val Acc: 0.7044922113418579\n",
      "Epoch: 58, Train_Loss: 0.4215465188026428, Train Acc: 0.8666978478431702, Val Acc: 0.7255859375\n",
      "Epoch: 59, Train_Loss: 0.43131789565086365, Train Acc: 0.8624243140220642, Val Acc: 0.7076171636581421\n",
      "Epoch: 60, Train_Loss: 0.41619911789894104, Train Acc: 0.8688568472862244, Val Acc: 0.721484363079071\n",
      "Epoch: 61, Train_Loss: 0.41829583048820496, Train Acc: 0.8668758869171143, Val Acc: 0.709179699420929\n",
      "Epoch: 62, Train_Loss: 0.4159092605113983, Train Acc: 0.8665865659713745, Val Acc: 0.7308593988418579\n",
      "Epoch: 63, Train_Loss: 0.4053281545639038, Train Acc: 0.871104896068573, Val Acc: 0.7289062738418579\n",
      "Epoch: 64, Train_Loss: 0.41831353306770325, Train Acc: 0.8666978478431702, Val Acc: 0.7162109613418579\n",
      "Epoch: 65, Train_Loss: 0.4133646786212921, Train Acc: 0.8694132566452026, Val Acc: 0.728320300579071\n",
      "Epoch: 66, Train_Loss: 0.41647014021873474, Train Acc: 0.8695468306541443, Val Acc: 0.720898449420929\n",
      "Epoch: 67, Train_Loss: 0.40557631850242615, Train Acc: 0.8730413317680359, Val Acc: 0.722851574420929\n",
      "Epoch: 68, Train_Loss: 0.4108828902244568, Train Acc: 0.8698806762695312, Val Acc: 0.719921886920929\n",
      "Epoch: 69, Train_Loss: 0.41007861495018005, Train Acc: 0.8701032996177673, Val Acc: 0.7201172113418579\n",
      "Epoch: 70, Train_Loss: 0.3978385329246521, Train Acc: 0.8752225637435913, Val Acc: 0.7173827886581421\n",
      "Epoch: 71, Train_Loss: 0.40208759903907776, Train Acc: 0.8754006624221802, Val Acc: 0.7109375\n",
      "Epoch: 72, Train_Loss: 0.4104941487312317, Train Acc: 0.8717281222343445, Val Acc: 0.6986328363418579\n",
      "Epoch: 73, Train_Loss: 0.39253395795822144, Train Acc: 0.8745103478431702, Val Acc: 0.71875\n",
      "Epoch: 74, Train_Loss: 0.4015843868255615, Train Acc: 0.8736867904663086, Val Acc: 0.708984375\n",
      "Epoch: 75, Train_Loss: 0.39525729417800903, Train Acc: 0.87589031457901, Val Acc: 0.7144531011581421\n",
      "Epoch: 76, Train_Loss: 0.40335503220558167, Train Acc: 0.8726851940155029, Val Acc: 0.7230468988418579\n",
      "Epoch: 77, Train_Loss: 0.40532299876213074, Train Acc: 0.8739316463470459, Val Acc: 0.7124999761581421\n",
      "Epoch: 78, Train_Loss: 0.4086993932723999, Train Acc: 0.8713496923446655, Val Acc: 0.7142578363418579\n",
      "Epoch: 79, Train_Loss: 0.39270511269569397, Train Acc: 0.8777377009391785, Val Acc: 0.7216796875\n",
      "Epoch: 80, Train_Loss: 0.4151264429092407, Train Acc: 0.8732861280441284, Val Acc: 0.699999988079071\n",
      "Epoch: 81, Train_Loss: 0.40035098791122437, Train Acc: 0.8764912486076355, Val Acc: 0.712695300579071\n",
      "Epoch: 82, Train_Loss: 0.4042946994304657, Train Acc: 0.8729300498962402, Val Acc: 0.708203136920929\n",
      "Epoch: 83, Train_Loss: 0.3944057822227478, Train Acc: 0.8758680820465088, Val Acc: 0.722460925579071\n",
      "Epoch: 84, Train_Loss: 0.4033472239971161, Train Acc: 0.8746216297149658, Val Acc: 0.712890625\n",
      "Epoch: 85, Train_Loss: 0.3937109410762787, Train Acc: 0.8800525069236755, Val Acc: 0.712109386920929\n",
      "Epoch: 86, Train_Loss: 0.4318300783634186, Train Acc: 0.8697471618652344, Val Acc: 0.7171875238418579\n",
      "Epoch: 87, Train_Loss: 0.4066859483718872, Train Acc: 0.8756899833679199, Val Acc: 0.714062511920929\n",
      "Epoch: 88, Train_Loss: 0.39721402525901794, Train Acc: 0.8778044581413269, Val Acc: 0.711132824420929\n",
      "Epoch: 89, Train_Loss: 0.40269050002098083, Train Acc: 0.8769586682319641, Val Acc: 0.696484386920929\n",
      "Epoch: 90, Train_Loss: 0.3870835602283478, Train Acc: 0.8817218542098999, Val Acc: 0.71875\n",
      "Epoch: 91, Train_Loss: 0.39995449781417847, Train Acc: 0.8786947727203369, Val Acc: 0.7109375\n",
      "Epoch: 92, Train_Loss: 0.40925151109695435, Train Acc: 0.8751112818717957, Val Acc: 0.7080078125\n",
      "Epoch: 93, Train_Loss: 0.3920844495296478, Train Acc: 0.8810763955116272, Val Acc: 0.7085937261581421\n",
      "Epoch: 94, Train_Loss: 0.38662320375442505, Train Acc: 0.881588339805603, Val Acc: 0.7181640863418579\n",
      "Epoch: 95, Train_Loss: 0.3973866105079651, Train Acc: 0.8802751302719116, Val Acc: 0.719531238079071\n",
      "Epoch: 96, Train_Loss: 0.38484522700309753, Train Acc: 0.8855056762695312, Val Acc: 0.710742175579071\n",
      "Epoch: 97, Train_Loss: 0.3949861526489258, Train Acc: 0.8810763955116272, Val Acc: 0.7142578363418579\n",
      "Epoch: 98, Train_Loss: 0.39706408977508545, Train Acc: 0.8782274127006531, Val Acc: 0.7083984613418579\n",
      "Epoch: 99, Train_Loss: 0.3961198627948761, Train Acc: 0.8808093070983887, Val Acc: 0.700390636920929\n"
     ]
    }
   ],
   "source": [
    "def train_loop(state,train_loader,val_loader,num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # train 1 epoch\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        for batch in train_loader:\n",
    "            state, loss, train_acc = train_step(state, batch)\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(loss)\n",
    "        train_acc = np.mean(train_accs)\n",
    "        loss = np.mean(train_losses)\n",
    "        \n",
    "        # eval\n",
    "        val_accs = []\n",
    "        for batch in val_loader:\n",
    "            val_acc = eval_step(state, batch)\n",
    "            val_accs.append(val_acc)\n",
    "        val_acc = np.mean(val_accs)\n",
    "                \n",
    "        # log\n",
    "        print(f\"Epoch: {epoch}, Train_Loss: {loss}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n",
    "\n",
    "train_loop(model_state, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 細々としたモジュールども"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "# https://flax.readthedocs.io/en/latest/guides/training_techniques/dropout.html\n",
    "\n",
    "# model\n",
    "# 別にcompactでもかける\n",
    "class DropoutMLP(nn.Module):\n",
    "    input_dim: int = 3\n",
    "    hidden_dim: int = 30\n",
    "    output_dim: int = 5\n",
    "    dropout_prob: float = 0.5\n",
    "    \n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.hidden_dim)\n",
    "        self.dense2 = nn.Dense(self.output_dim)\n",
    "        self.dropout = nn.Dropout(rate=self.dropout_prob)\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22760291  0.10780797 -0.01367897  0.9634542  -0.0916012 ]\n",
      " [ 0.5429692   0.5138054  -0.06723279  0.87348104  0.7162861 ]\n",
      " [ 0.6461147   0.00951485  0.43068302  1.9464691  -0.17464459]]\n",
      "[[0.896132   0.28386933 0.0984112  1.3529255  0.20996922]\n",
      " [0.896132   0.28386933 0.0984112  1.3529255  0.20996922]\n",
      " [0.896132   0.28386933 0.0984112  1.3529255  0.20996922]]\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "key, params_key = jax.random.split(key)\n",
    "model = DropoutMLP(dropout_prob=0.3)\n",
    "input_shape = jnp.empty((1, 3))\n",
    "variables = model.init(params_key, x, training=False)\n",
    "params = variables['params']\n",
    "\n",
    "# forward with dropout\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, dropout_key = jax.random.split(key)\n",
    "x = jnp.ones((3, 3))\n",
    "y = model.apply({\"params\": params}, x, training=True, rngs={\"dropout\": dropout_key})\n",
    "print(y)\n",
    "\n",
    "# forward without dropout\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, dropout_key = jax.random.split(key)\n",
    "x = jnp.ones((3, 3))\n",
    "#y = model.apply({\"params\": params}, x, training=False, rngs={\"dropout\": dropout_key})\n",
    "y = model.apply({\"params\": params}, x) # trainingをデフォルトでFalseにしておくと、これで良くなる\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected function with aux output to return a two-element tuple, but got type <class 'jax._src.interpreters.ad.JVPTracer'> with value Traced<ConcreteArray(1.5074005126953125, dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array(1.5074005, dtype=float32)\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f007e950130>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>,), out_tracer_refs=[<weakref at 0x7eff30620b80; to 'JaxprTracer' at 0x7eff3075c0e0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3]. let\n    b:f32[] = reduce_sum[axes=(0,)] a\n    c:f32[] = div b 3.0\n  in (c,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False,), 'name': '_mean', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f007e92f8f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m labels \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     39\u001b[0m batch \u001b[38;5;241m=\u001b[39m (data, labels)\n\u001b[0;32m---> 40\u001b[0m state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 32\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#(loss, acc), grads = jax.value_and_grad(loss_fn, has_aux=True)(state, state.params, batch)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.10/site-packages/jax/_src/api_util.py:143\u001b[0m, in \u001b[0;36mflatten_fun_nokwargs2\u001b[0;34m(in_tree, *args_flat)\u001b[0m\n\u001b[1;32m    141\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m py_args, {}\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pair, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected function with aux output to return a two-element \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuple, but got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m ans, aux \u001b[38;5;241m=\u001b[39m pair\n\u001b[1;32m    146\u001b[0m ans_flat, ans_tree \u001b[38;5;241m=\u001b[39m tree_flatten(ans)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected function with aux output to return a two-element tuple, but got type <class 'jax._src.interpreters.ad.JVPTracer'> with value Traced<ConcreteArray(1.5074005126953125, dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array(1.5074005, dtype=float32)\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f007e950130>, in_tracers=(Traced<ShapedArray(float32[3]):JaxprTrace(level=1/0)>,), out_tracer_refs=[<weakref at 0x7eff30620b80; to 'JaxprTracer' at 0x7eff3075c0e0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[3]. let\n    b:f32[] = reduce_sum[axes=(0,)] a\n    c:f32[] = div b 3.0\n  in (c,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False,), 'name': '_mean', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f007e92f8f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))"
     ]
    }
   ],
   "source": [
    "# flax training \n",
    "class TrainState(train_state.TrainState):\n",
    "    dropout_key: jax.Array\n",
    "\n",
    "dropout_key = jax.random.PRNGKey(0)\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    dropout_key=dropout_key,\n",
    "    tx=optax.adam(1e-3)\n",
    ")\n",
    "\n",
    "# train step\n",
    "def loss_fn(\n",
    "    state: TrainState,\n",
    "    params: dict,\n",
    "    batch: jnp.ndarray,\n",
    "    training: bool,\n",
    "    dropout_key: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Computes the loss for a single batch.\"\"\"\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": params}, data, training=True, rngs={\"dropout\": dropout_key})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "    return loss\n",
    "\n",
    "#@jax.jit\n",
    "def train_step(state, batch):\n",
    "    dropout_key = jax.random.fold_in(state.dropout_key, state.step)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, argnums=1, has_aux=True)\n",
    "    #(loss, acc), grads = jax.value_and_grad(loss_fn, has_aux=True)(state, state.params, batch)\n",
    "    loss, grads = grad_fn(state, state.params, batch, True, dropout_key)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# test one step\n",
    "data = jnp.ones((3, 3))\n",
    "labels = jnp.array([0, 1, 2])\n",
    "batch = (data, labels)\n",
    "state, loss = train_step(state, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
